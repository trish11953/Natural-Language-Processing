{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1          by Trisha Mandal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/trishamandal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from bs4) (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (0.1.72)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the given dataset by using pd.read_table\n",
    "df = pd.read_table('amazon_reviews_us_Jewelry_v1_00.tsv', error_bad_lines=False,warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have saved only the star_ratings and review_body in a dataframe.\n",
    "data = pd.concat([df['star_rating'], df['review_body']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 20000 reviews randomly from each rating class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have selected 20000 ratings from each rating category to have balanced data by using .sample method on the \n",
    "# dataframe and then combining 20000 reviews of each rating into 1 dataframe \n",
    "data = pd.concat([df['star_rating'], df['review_body']], axis=1)\n",
    "data = data.dropna()\n",
    "#data.drop(data[data['review_body'].str.split().str.len() < 10].index, inplace=True)\n",
    "data['star_rating']= [int(i) for i in data['star_rating']]\n",
    "rating1 = data[data['star_rating'] == 1].sample(n=20000, random_state = 2)\n",
    "rating2 = data[data['star_rating'] == 2].sample(n=20000, random_state = 2)\n",
    "rating3 = data[data['star_rating'] == 3].sample(n=20000, random_state = 2)\n",
    "rating4 = data[data['star_rating'] == 4].sample(n=20000, random_state = 2)\n",
    "rating5 = data[data['star_rating'] == 5].sample(n=20000, random_state = 2)\n",
    "combined = pd.concat([rating1, rating2, rating3, rating4, rating5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping a copy of the combined dataframe after sampling\n",
    "combinedcopy = combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all the reviews into string\n",
    "combined['review_body']= [str(i) for i in combined['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average length of words before data cleaning by using mean on length of string\n",
    "avg = combined['review_body'].apply(lambda x: len(x))\n",
    "avgwordlength1 = avg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948126</th>\n",
       "      <td>1</td>\n",
       "      <td>i was disappointed with the quality and the si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32115</th>\n",
       "      <td>1</td>\n",
       "      <td>cheap and ugly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752868</th>\n",
       "      <td>1</td>\n",
       "      <td>do not waste your money depicts silver studs b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155702</th>\n",
       "      <td>1</td>\n",
       "      <td>if you are toying with the idea of buying this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908775</th>\n",
       "      <td>1</td>\n",
       "      <td>way too long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330384</th>\n",
       "      <td>5</td>\n",
       "      <td>this charm fits great on my pandora bracelet a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603227</th>\n",
       "      <td>5</td>\n",
       "      <td>i am so glad that i purchased this bracelet i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025288</th>\n",
       "      <td>5</td>\n",
       "      <td>bought this as a gift for a friend he absolute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522848</th>\n",
       "      <td>5</td>\n",
       "      <td>this is quite a find especially not because of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661703</th>\n",
       "      <td>5</td>\n",
       "      <td>looks good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "948126             1  i was disappointed with the quality and the si...\n",
       "32115              1                                     cheap and ugly\n",
       "752868             1  do not waste your money depicts silver studs b...\n",
       "1155702            1  if you are toying with the idea of buying this...\n",
       "908775             1                                       way too long\n",
       "...              ...                                                ...\n",
       "1330384            5  this charm fits great on my pandora bracelet a...\n",
       "1603227            5  i am so glad that i purchased this bracelet i ...\n",
       "1025288            5  bought this as a gift for a friend he absolute...\n",
       "1522848            5  this is quite a find especially not because of...\n",
       "661703             5                                         looks good\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data cleaning on sampled data\n",
    "# step 1: changing all words to lower case by using str.lower()\n",
    "# step 2: performing contractions on the reviews by using contractions library\n",
    "# step 3: Removing HTML by using BeatifulSoup library\n",
    "# step 4: Removing URLS by using regex\n",
    "# step 5: Removing non-alphanumeric characters by using regex\n",
    "# step 6: Stripping extra space\n",
    "# step 7: Replacing double spaces with single spaces\n",
    "import contractions\n",
    "combined['review_body'] = combined['review_body'].str.lower()\n",
    "combined['review_body'] = combined['review_body'].apply(lambda x: contractions.fix(x))\n",
    "combined['review_body'] = [BeautifulSoup(text).get_text() for text in combined['review_body']]\n",
    "combined['review_body'] = combined['review_body'].apply(lambda text: re.sub(r'www.\\S+.com', '', text))\n",
    "combined['review_body'] = combined['review_body'].apply(lambda text: re.sub(r'https?:\\S+', '', text)) \n",
    "combined['review_body'] = combined['review_body'].apply(lambda x: re.sub('\\W+', ' ', x))\n",
    "combined[\"review_body\"] = combined[\"review_body\"].apply(lambda x: re.sub(r\"\\d+\", ' ', x))\n",
    "combined[\"review_body\"] = combined[\"review_body\"].apply(lambda x: x.strip()) \n",
    "combined[\"review_body\"] = combined[\"review_body\"].apply(lambda x: x.replace(\"  \", \" \"))\n",
    "#combined[\"review_body\"] = combined[\"review_body\"].apply(lambda x: TextBlob(x).correct())\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review Length before and after data cleaning: 189.88698, 183.31113 \n"
     ]
    }
   ],
   "source": [
    "#Average length of words after data cleaning by using mean on length of string\n",
    "avg = combined['review_body'].apply(lambda x: len(x))\n",
    "avgwordlength2 = avg.mean()\n",
    "print(\"Average Review Length before and after data cleaning: {}, {} \".format(avgwordlength1, avgwordlength2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words by using stopwords from nltk library\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "combined['review_body'] = combined['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average length of words after removing stop words by using mean on length of string\n",
    "avg = combined['review_body'].apply(lambda x: len(x))\n",
    "avgwordlength3 = avg.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/trishamandal/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# performing lemmatization step using WordNetLemmatizer as given\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "combined['review_body'] = combined['review_body'].apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review Length before and after preprocessing: 109.8307, 109.83013\n"
     ]
    }
   ],
   "source": [
    "#Average length of words after lemmatization by using mean on length of string\n",
    "avg = combined['review_body'].apply(lambda x: len(x))\n",
    "avgwordlength4 = avg.mean()\n",
    "print(\"Average Review Length before and after preprocessing: {}, {}\".format(avgwordlength3, avgwordlength4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF-IDF for feature extraction and vectorizing the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vec = vectorizer.fit_transform(combined['review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and test set from train_test_split (80-20 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "labels = combined['star_rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(vec,combined,test_size=0.2,random_state=2,stratify=labels)\n",
    "ytest = ytest['star_rating']\n",
    "ytrain = ytrain['star_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for every class:  [0.56    0.209   0.28025 0.4245  0.544  ]\n",
      "F1 score for every class:  [0.51352591 0.25770654 0.2851329  0.3714317  0.55923927]\n",
      "Precision score for every class:  [0.47417443 0.33601286 0.29018897 0.3301575  0.57535695]\n",
      "Recall score:  0.40354999999999996\n",
      "F1 score:  0.39740726162167367\n",
      "Precision score:  0.4011781424211677\n",
      "Accuracy score:  0.40355\n"
     ]
    }
   ],
   "source": [
    "# training the Perceptron model on training data set and then calculating precision score, recall score and accuracy\n",
    "# score by calling sklearn.metrics functions to calculate the values and then the average of these values\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clr = Perceptron(random_state=2)\n",
    "clr = clr.fit(xtrain, ytrain)\n",
    "prediction = clr.predict(xtest)\n",
    "recall = recall_score(ytest, prediction, average=None)\n",
    "f1 = f1_score(ytest, prediction, average=None)\n",
    "precision = precision_score(ytest, prediction, average=None)\n",
    "acc = accuracy_score(ytest, prediction)\n",
    "print(\"Recall score for every class: \", recall)\n",
    "print(\"F1 score for every class: \", f1)\n",
    "print(\"Precision score for every class: \", precision)\n",
    "print(\"Recall score: \", recall.mean())\n",
    "print(\"F1 score: \", f1.mean())\n",
    "print(\"Precision score: \", precision.mean())\n",
    "print(\"Accuracy score: \", acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for every class:  [0.63675 0.33075 0.33475 0.4015  0.71925]\n",
      "F1 score for every class:  [0.58754325 0.35060289 0.3630202  0.41828363 0.6593331 ]\n",
      "Precision score for every class:  [0.54539615 0.37299126 0.39650577 0.43653167 0.60863127]\n",
      "Recall score:  0.48460000000000003\n",
      "F1 score:  0.4757566147826324\n",
      "Precision score:  0.4720112227176513\n",
      "Accuracy score:  0.4846\n"
     ]
    }
   ],
   "source": [
    "# training the LinearSVC model on training data set and then calculating precision score, recall score and accuracy\n",
    "# score by calling sklearn.metrics functions to calculate the values and then the average of these values\n",
    "from sklearn.svm import LinearSVC\n",
    "clr2 = LinearSVC(random_state=2)\n",
    "clr2.fit(xtrain, ytrain)\n",
    "prediction = clr2.predict(xtest)\n",
    "recall = recall_score(ytest, prediction, average=None)\n",
    "f1 = f1_score(ytest, prediction, average=None)\n",
    "precision = precision_score(ytest, prediction, average=None)\n",
    "acc = accuracy_score(ytest, prediction)\n",
    "print(\"Recall score for every class: \", recall)\n",
    "print(\"F1 score for every class: \", f1)\n",
    "print(\"Precision score for every class: \", precision)\n",
    "print(\"Recall score: \", recall.mean())\n",
    "print(\"F1 score: \", f1.mean())\n",
    "print(\"Precision score: \", precision.mean())\n",
    "print(\"Accuracy score: \", acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for every class:  [0.61575 0.3935  0.3925  0.443   0.70625]\n",
      "F1 score for every class:  [0.60286379 0.40405596 0.4005102  0.45558555 0.67110108]\n",
      "Precision score for every class:  [0.59050587 0.41519388 0.40885417 0.46890712 0.63928491]\n",
      "Recall score:  0.5102\n",
      "F1 score:  0.5068233168763573\n",
      "Precision score:  0.5045491890346222\n",
      "Accuracy score:  0.5102\n"
     ]
    }
   ],
   "source": [
    "# training the LogisticRegression model on training data set and then calculating precision score, recall score and\n",
    "# accuracy score by calling sklearn.metrics functions to calculate the values and then the average of these values\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clr3 = LogisticRegression(random_state=0).fit(xtrain, ytrain)\n",
    "prediction = clr3.predict(xtest)\n",
    "recall = recall_score(ytest, prediction, average=None)\n",
    "f1 = f1_score(ytest, prediction, average=None)\n",
    "precision = precision_score(ytest, prediction, average=None)\n",
    "print(\"Recall score for every class: \", recall)\n",
    "print(\"F1 score for every class: \", f1)\n",
    "print(\"Precision score for every class: \", precision)\n",
    "acc = accuracy_score(ytest, prediction)\n",
    "print(\"Recall score: \", recall.mean())\n",
    "print(\"F1 score: \", f1.mean())\n",
    "print(\"Precision score: \", precision.mean())\n",
    "print(\"Accuracy score: \", acc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score for every class:  [0.60025 0.38275 0.39575 0.425   0.67825]\n",
      "F1 score for every class:  [0.59108813 0.38976578 0.39466467 0.43584156 0.66194949]\n",
      "Precision score for every class:  [0.58220175 0.39704357 0.39358528 0.44725072 0.64641411]\n",
      "Recall score:  0.49640000000000006\n",
      "F1 score:  0.4946619285363082\n",
      "Precision score:  0.4932990848208808\n",
      "Accuracy score:  0.4964\n"
     ]
    }
   ],
   "source": [
    "# training the MultinomialNB model on training data set and then calculating precision score, recall score and accuracy\n",
    "# score by calling sklearn.metrics functions to calculate the values and then the average of these values\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clr4 = MultinomialNB().fit(xtrain, ytrain)\n",
    "prediction = clr4.predict(xtest)\n",
    "recall = recall_score(ytest, prediction, average=None)\n",
    "f1 = f1_score(ytest, prediction, average=None)\n",
    "precision = precision_score(ytest, prediction, average=None)\n",
    "print(\"Recall score for every class: \", recall)\n",
    "print(\"F1 score for every class: \", f1)\n",
    "print(\"Precision score for every class: \", precision)\n",
    "acc = accuracy_score(ytest, prediction)\n",
    "print(\"Recall score: \", recall.mean())\n",
    "print(\"F1 score: \", f1.mean())\n",
    "print(\"Precision score: \", precision.mean())\n",
    "print(\"Accuracy score: \", acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
