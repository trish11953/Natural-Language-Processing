{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a38ab8",
   "metadata": {},
   "source": [
    "# HW2 by Trisha Mandal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1a7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/trishamandal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d30124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/trishamandal/opt/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447aa96",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79e2f6",
   "metadata": {},
   "source": [
    "We will use the Amazon reviews dataset used in HW1. Load the dataset and\n",
    "build a balanced dataset of 100K reviews along with their ratings to create\n",
    "labels through random selection. You can store your dataset after generation\n",
    "and reuse it to reduce the computational load. For your experiments consider\n",
    "a 80%/20% training/testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337f7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "df = pd.read_table('amazon_reviews_us_Jewelry_v1_00.tsv', error_bad_lines=False,warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2c7e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only reviews and ratings \n",
    "data = pd.concat([df['star_rating'], df['review_body']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be409a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all reviews into string\n",
    "data['review_body']= [str(i) for i in data['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5845b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning on sampled data\n",
    "# step 1: changing all words to lower case by using str.lower()\n",
    "# step 2: performing contractions on the reviews by using contractions library\n",
    "# step 3: Removing HTML by using BeatifulSoup library\n",
    "# step 4: Removing URLS by using regex\n",
    "# step 5: Removing non-alphanumeric characters by using regex\n",
    "# step 6: Stripping extra space\n",
    "# step 7: Replacing double spaces with single spaces\n",
    "import contractions\n",
    "data.dropna()\n",
    "data['review_body'] = data['review_body'].str.lower()\n",
    "# data.drop(data[data['review_body'].str.split().str.len() < 10].index, inplace=True)\n",
    "data['review_body'] = data['review_body'].apply(lambda x: contractions.fix(x))\n",
    "data['review_body'] = [BeautifulSoup(text).get_text() for text in data['review_body']]\n",
    "data['review_body'] = data['review_body'].apply(lambda text: re.sub(r'www.\\S+.com', '', text))\n",
    "data['review_body'] = data['review_body'].apply(lambda text: re.sub(r'https?:\\S+', '', text)) \n",
    "data['review_body'] = data['review_body'].apply(lambda x: re.sub('\\W+', ' ', x))\n",
    "data[\"review_body\"] = data[\"review_body\"].apply(lambda x: re.sub(r\"\\d+\", ' ', x))\n",
    "data[\"review_body\"] = data[\"review_body\"].apply(lambda x: x.strip()) \n",
    "data[\"review_body\"] = data[\"review_body\"].apply(lambda x: x.replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152bdf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>not what i expected took too long to ship but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>it states that the item is new but you cannot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>the bracelet did not fit properly i had to act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>3</td>\n",
       "      <td>i have never been able to use these on my ring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "      <td>arrived broken one if the stones were out look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766904</th>\n",
       "      <td>3</td>\n",
       "      <td>the braclet and ring set of infinite potential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766930</th>\n",
       "      <td>3</td>\n",
       "      <td>well i got this necklace and come on its   ctw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766952</th>\n",
       "      <td>3</td>\n",
       "      <td>less is best i may buy the jewelry but i am no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766955</th>\n",
       "      <td>3</td>\n",
       "      <td>the earrings are very pretty but they were too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766962</th>\n",
       "      <td>3</td>\n",
       "      <td>just wanted to let buyers know that these earr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153665 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body\n",
       "17                3  not what i expected took too long to ship but ...\n",
       "35                3  it states that the item is new but you cannot ...\n",
       "40                3  the bracelet did not fit properly i had to act...\n",
       "83                3  i have never been able to use these on my ring...\n",
       "88                3  arrived broken one if the stones were out look...\n",
       "...             ...                                                ...\n",
       "1766904           3  the braclet and ring set of infinite potential...\n",
       "1766930           3  well i got this necklace and come on its   ctw...\n",
       "1766952           3  less is best i may buy the jewelry but i am no...\n",
       "1766955           3  the earrings are very pretty but they were too...\n",
       "1766962           3  just wanted to let buyers know that these earr...\n",
       "\n",
       "[153665 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seperating reviews from each rating\n",
    "star1 = data[data['star_rating'] == 1]\n",
    "star2 = data[data['star_rating'] == 2]\n",
    "star3 = data[data['star_rating'] == 3]\n",
    "star4 = data[data['star_rating'] == 4]\n",
    "star5 = data[data['star_rating'] == 5]\n",
    "star3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5915a2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>so beautiful even though clearly not high end ...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>great product i got this set for my mother as ...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>exactly as pictured and my daughter s friend l...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>love it fits great super comfortable and neat ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>got this as a mother s day gift for my mom and...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766982</th>\n",
       "      <td>5</td>\n",
       "      <td>i love these earings my boyfriend got me a pai...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766983</th>\n",
       "      <td>5</td>\n",
       "      <td>not too much money but would make a good impre...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766985</th>\n",
       "      <td>5</td>\n",
       "      <td>i was so impressed with this piece i am a jewe...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766990</th>\n",
       "      <td>5</td>\n",
       "      <td>the kt gold earrings look remarkable would def...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766991</th>\n",
       "      <td>5</td>\n",
       "      <td>it will be a gift to my special friend we know...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1041018 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  length\n",
       "0                 5  so beautiful even though clearly not high end ...      32\n",
       "1                 5  great product i got this set for my mother as ...      72\n",
       "2                 5  exactly as pictured and my daughter s friend l...      32\n",
       "3                 5  love it fits great super comfortable and neat ...      18\n",
       "4                 5  got this as a mother s day gift for my mom and...      22\n",
       "...             ...                                                ...     ...\n",
       "1766982           5  i love these earings my boyfriend got me a pai...      39\n",
       "1766983           5  not too much money but would make a good impre...      10\n",
       "1766985           5  i was so impressed with this piece i am a jewe...      84\n",
       "1766990           5  the kt gold earrings look remarkable would def...      14\n",
       "1766991           5  it will be a gift to my special friend we know...      46\n",
       "\n",
       "[1041018 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculated review length for each document and putting it in length column in dataframe\n",
    "numofwords1 = star1[\"review_body\"].apply(lambda x: len(str(x).split(' ')))\n",
    "star1[\"length\"] = pd.DataFrame(numofwords1)\n",
    "numofwords2 = star2[\"review_body\"].apply(lambda x: len(str(x).split(' ')))\n",
    "star2[\"length\"] = pd.DataFrame(numofwords2)\n",
    "numofwords3 = star3[\"review_body\"].apply(lambda x: len(str(x).split(' ')))\n",
    "star3[\"length\"] = pd.DataFrame(numofwords3)\n",
    "numofwords4 = star4[\"review_body\"].apply(lambda x: len(str(x).split(' ')))\n",
    "star4[\"length\"] = pd.DataFrame(numofwords4)\n",
    "numofwords5 = star5[\"review_body\"].apply(lambda x: len(str(x).split(' ')))\n",
    "star5[\"length\"] = pd.DataFrame(numofwords5)\n",
    "star5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fecd9e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1648184</th>\n",
       "      <td>1.0</td>\n",
       "      <td>the entire review history is below but my most...</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431879</th>\n",
       "      <td>1</td>\n",
       "      <td>the post broke off the face while trying to ge...</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216795</th>\n",
       "      <td>1</td>\n",
       "      <td>i am writing a review for the three rings i pu...</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757958</th>\n",
       "      <td>1</td>\n",
       "      <td>are  of the sales profits of these red strings...</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i bought this bracelet as a christmas gift for...</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740425</th>\n",
       "      <td>1</td>\n",
       "      <td>terrible</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238902</th>\n",
       "      <td>1</td>\n",
       "      <td>garbage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743657</th>\n",
       "      <td>1</td>\n",
       "      <td>awful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278586</th>\n",
       "      <td>1</td>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884542</th>\n",
       "      <td>1</td>\n",
       "      <td>junk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150461 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  length\n",
       "1648184         1.0  the entire review history is below but my most...    1489\n",
       "1431879           1  the post broke off the face while trying to ge...    1139\n",
       "1216795           1  i am writing a review for the three rings i pu...    1075\n",
       "1757958           1  are  of the sales profits of these red strings...    1035\n",
       "1687894         1.0  i bought this bracelet as a christmas gift for...     952\n",
       "...             ...                                                ...     ...\n",
       "740425            1                                           terrible       1\n",
       "238902            1                                            garbage       1\n",
       "743657            1                                              awful       1\n",
       "278586            1                                               huge       1\n",
       "884542            1                                               junk       1\n",
       "\n",
       "[150461 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting datframes in descending order by review length\n",
    "star1 = star1.sort_values(by='length', ascending=False)\n",
    "star2 = star2.sort_values(by='length', ascending=False)\n",
    "star3 = star3.sort_values(by='length', ascending=False)\n",
    "star4 = star4.sort_values(by='length', ascending=False)\n",
    "star5 = star5.sort_values(by='length', ascending=False)\n",
    "star1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8355ac9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1648184</th>\n",
       "      <td>1.0</td>\n",
       "      <td>the entire review history is below but my most...</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431879</th>\n",
       "      <td>1</td>\n",
       "      <td>the post broke off the face while trying to ge...</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216795</th>\n",
       "      <td>1</td>\n",
       "      <td>i am writing a review for the three rings i pu...</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757958</th>\n",
       "      <td>1</td>\n",
       "      <td>are  of the sales profits of these red strings...</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i bought this bracelet as a christmas gift for...</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964376</th>\n",
       "      <td>1</td>\n",
       "      <td>i recieved this product a day earlier that is ...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222447</th>\n",
       "      <td>1</td>\n",
       "      <td>i purchased this necklace to wear with an outf...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827380</th>\n",
       "      <td>1</td>\n",
       "      <td>i bought this because it looked like a great c...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247483</th>\n",
       "      <td>1</td>\n",
       "      <td>i had bought this product based on the picture...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680368</th>\n",
       "      <td>1.0</td>\n",
       "      <td>the image of the pendant is quite beautiful bu...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  length\n",
       "1648184         1.0  the entire review history is below but my most...    1489\n",
       "1431879           1  the post broke off the face while trying to ge...    1139\n",
       "1216795           1  i am writing a review for the three rings i pu...    1075\n",
       "1757958           1  are  of the sales profits of these red strings...    1035\n",
       "1687894         1.0  i bought this bracelet as a christmas gift for...     952\n",
       "...             ...                                                ...     ...\n",
       "964376            1  i recieved this product a day earlier that is ...      65\n",
       "1222447           1  i purchased this necklace to wear with an outf...      65\n",
       "827380            1  i bought this because it looked like a great c...      65\n",
       "1247483           1  i had bought this product based on the picture...      65\n",
       "1680368         1.0  the image of the pendant is quite beautiful bu...      65\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking top 20K reviews from each dataframe\n",
    "star1 = star1.head(20000)\n",
    "star2 = star2.head(20000)\n",
    "star3 = star3.head(20000)\n",
    "star4 = star4.head(20000)\n",
    "star5 = star5.head(20000)\n",
    "star1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19556eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1648184</th>\n",
       "      <td>1.0</td>\n",
       "      <td>the entire review history is below but my most...</td>\n",
       "      <td>1489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431879</th>\n",
       "      <td>1</td>\n",
       "      <td>the post broke off the face while trying to ge...</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216795</th>\n",
       "      <td>1</td>\n",
       "      <td>i am writing a review for the three rings i pu...</td>\n",
       "      <td>1075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757958</th>\n",
       "      <td>1</td>\n",
       "      <td>are  of the sales profits of these red strings...</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>i bought this bracelet as a christmas gift for...</td>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596083</th>\n",
       "      <td>5.0</td>\n",
       "      <td>i bought this ring as my wedding band thinking...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715843</th>\n",
       "      <td>5</td>\n",
       "      <td>this ring is my new engagement wedding ring an...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318493</th>\n",
       "      <td>5</td>\n",
       "      <td>this ring is gorgeous my diamond fell out of m...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593443</th>\n",
       "      <td>5.0</td>\n",
       "      <td>i adore these earrings i also ordered the coor...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950317</th>\n",
       "      <td>5</td>\n",
       "      <td>this like the  blue hearts opal ring i am cond...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                        review_body  length\n",
       "1648184         1.0  the entire review history is below but my most...    1489\n",
       "1431879           1  the post broke off the face while trying to ge...    1139\n",
       "1216795           1  i am writing a review for the three rings i pu...    1075\n",
       "1757958           1  are  of the sales profits of these red strings...    1035\n",
       "1687894         1.0  i bought this bracelet as a christmas gift for...     952\n",
       "...             ...                                                ...     ...\n",
       "1596083         5.0  i bought this ring as my wedding band thinking...     131\n",
       "1715843           5  this ring is my new engagement wedding ring an...     131\n",
       "1318493           5  this ring is gorgeous my diamond fell out of m...     131\n",
       "1593443         5.0  i adore these earrings i also ordered the coor...     131\n",
       "950317            5  this like the  blue hearts opal ring i am cond...     131\n",
       "\n",
       "[100000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining all rating dataframes\n",
    "combined2 = pd.concat([star1, star2, star3, star4,star5])\n",
    "combined2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "55df8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined2['review_body']= [str(i) for i in combined2['review_body']]\n",
    "combined2['star_rating']= [int(i) for i in combined2['star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548cc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating all reviews using ratings\n",
    "rating1 = data[data['star_rating'] == 1]\n",
    "rating2 = data[data['star_rating'] == 2]\n",
    "rating3 = data[data['star_rating'] == 3]\n",
    "rating4 = data[data['star_rating'] == 4]\n",
    "# done random sampling for rating 5 since number of reviews are too large\n",
    "rating5 = rating5 = data[data['star_rating'] == 5].sample(n=20000, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc369d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertings reviews and ratings columns to lists for all rating categories\n",
    "re1 = rating1['review_body'].values.tolist()\n",
    "re2 = rating2['review_body'].values.tolist()\n",
    "re3 = rating3['review_body'].values.tolist()\n",
    "re4 = rating4['review_body'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f6add12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev1 = np.array(re1)\n",
    "rev2 = np.array(re2)\n",
    "rev3 = np.array(re3)\n",
    "rev4 = np.array(re4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "855c3d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying tfidf on all ratings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v1 = TfidfVectorizer()\n",
    "v2 = TfidfVectorizer()\n",
    "v3 = TfidfVectorizer()\n",
    "v4 = TfidfVectorizer()\n",
    "vec1 = v1.fit_transform(re1)\n",
    "vec2 = v2.fit_transform(re2)\n",
    "vec3 = v3.fit_transform(re3)\n",
    "vec4 = v4.fit_transform(re4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84642543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to numpy \n",
    "num1 = list(np.squeeze(np.asarray(np.sum(vec1, axis = 1).astype(np.float32))))\n",
    "num2 = list(np.squeeze(np.asarray(np.sum(vec2, axis = 1).astype(np.float32))))\n",
    "num3 = list(np.squeeze(np.asarray(np.sum(vec3, axis = 1).astype(np.float32))))\n",
    "num4 = list(np.squeeze(np.asarray(np.sum(vec4, axis = 1).astype(np.float32))))\n",
    "\n",
    "num1 = np.array(num1)\n",
    "num2 = np.array(num2)\n",
    "num3 = np.array(num3)\n",
    "num4 = np.array(num4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0b1614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshapping nparray\n",
    "num1 = num1.reshape((num1.shape[0], 1))\n",
    "num2 = num2.reshape((num2.shape[0], 1))\n",
    "num3 = num3.reshape((num3.shape[0], 1))\n",
    "num4 = num4.reshape((num4.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c877e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshapping nparray\n",
    "rev1 = rev1.reshape((rev1.shape[0], 1))\n",
    "rev2 = rev2.reshape((rev2.shape[0], 1))\n",
    "rev3 = rev3.reshape((rev3.shape[0], 1))\n",
    "rev4 = rev4.reshape((rev4.shape[0], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e9f20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding tfidf scores to dataframes\n",
    "df1 = pd.DataFrame(np.hstack((num1, rev1)), columns = ['tfidf', 'review_body'])\n",
    "df2 = pd.DataFrame(np.hstack((num2, rev2)), columns = ['tfidf', 'review_body'])\n",
    "df3 = pd.DataFrame(np.hstack((num3, rev3)), columns = ['tfidf', 'review_body'])\n",
    "df4 = pd.DataFrame(np.hstack((num4, rev4)), columns = ['tfidf', 'review_body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d67ddc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts tfidf vectors to numeric form\n",
    "df1['tfidf'] = pd.to_numeric(df1['tfidf'])\n",
    "df2['tfidf'] = pd.to_numeric(df2['tfidf'])\n",
    "df3['tfidf'] = pd.to_numeric(df3['tfidf'])\n",
    "df4['tfidf'] = pd.to_numeric(df4['tfidf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "774fb159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting length of reviews\n",
    "df1['Length'] = df1['review_body'].str.len()\n",
    "df2['Length'] = df2['review_body'].str.len()\n",
    "df3['Length'] = df3['review_body'].str.len()\n",
    "df4['Length'] = df4['review_body'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30fe5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking reviews according to how many times the words have appeared in a sentence\n",
    "df1['rank'] = df1['tfidf']/df1['Length']\n",
    "df2['rank'] = df2['tfidf']/df2['Length']\n",
    "df3['rank'] = df3['tfidf']/df3['Length']\n",
    "df4['rank'] = df4['tfidf']/df4['Length']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc23ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting them according to the ranks\n",
    "df1 = df1.sort_values(by='rank', ascending=False)\n",
    "df2 = df2.sort_values(by='rank', ascending=False)\n",
    "df3 = df3.sort_values(by='rank', ascending=False)\n",
    "df4 = df4.sort_values(by='rank', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3c4578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub sampling the highest 20000 ranks from each rating\n",
    "df1 = df1.head(20000)\n",
    "df2 = df2.head(20000)\n",
    "df3 = df3.head(20000)\n",
    "df4 = df4.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60fab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all unrequired columns for models in the later part of the project\n",
    "df1 = pd.DataFrame(df1['review_body']).reset_index(drop=True)\n",
    "df2 = pd.DataFrame(df2['review_body']).reset_index(drop=True)\n",
    "df3 = pd.DataFrame(df3['review_body']).reset_index(drop=True)\n",
    "df4 = pd.DataFrame(df4['review_body']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0abd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels lists\n",
    "    \n",
    "labels1 = [1]*20000\n",
    "labels2 = [2]*20000\n",
    "labels3 = [3]*20000\n",
    "labels4 = [4]*20000\n",
    "labels5 = [5]*20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7678320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from list to DF\n",
    "l1,l2,l3,l4,l5 = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame() \n",
    "l1['star_rating'] = pd.DataFrame(labels1) \n",
    "l2['star_rating'] = pd.DataFrame(labels2) \n",
    "l3['star_rating'] = pd.DataFrame(labels3) \n",
    "l4['star_rating']= pd.DataFrame(labels4) \n",
    "l5['star_rating']= pd.DataFrame(labels5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db212df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame(rating5['review_body']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcf96b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all dataframes\n",
    "frame_combined = pd.concat([df1,df2,df3,df4,df5])\n",
    "label_combined = pd.concat([l1,l2,l3,l4,l5])\n",
    "combined = pd.concat([frame_combined, label_combined], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f18e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([df['star_rating'], df['review_body']], axis=1)\n",
    "# data = data.dropna()\n",
    "# #data.drop(data[data['review_body'].str.split().str.len() < 10].index, inplace=True)\n",
    "# data['star_rating']= [int(i) for i in data['star_rating']]\n",
    "# rating1 = data[data['star_rating'] == 1].sample(n=20000, random_state = 2)\n",
    "# rating2 = data[data['star_rating'] == 2].sample(n=20000, random_state = 2)\n",
    "# rating3 = data[data['star_rating'] == 3].sample(n=20000, random_state = 2)\n",
    "# rating4 = data[data['star_rating'] == 4].sample(n=20000, random_state = 2)\n",
    "# rating5 = data[data['star_rating'] == 5].sample(n=20000, random_state = 2)\n",
    "# combined = pd.concat([rating1, rating2, rating3, rating4, rating5])\n",
    "# combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ec1ac",
   "metadata": {},
   "source": [
    "# 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088da6e",
   "metadata": {},
   "source": [
    "(a) Load the pretrained “word2vec-google-news-300” Word2Vec model and learn\n",
    "how to extract word embeddings for your dataset. Try to check semantic\n",
    "similarities of the generated vectors using three examples of your own, e.g.,\n",
    "King − M an + W oman = Queen or excellent ∼ outstanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0fd98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading all google word2vec vectors\n",
    "import gensim.downloader as api\n",
    "googlew2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97f32bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word and its similarity value:  [('golden_retriever', 0.8104889392852783)]\n"
     ]
    }
   ],
   "source": [
    "#checking similarity between the 2 words using built in function\n",
    "print(\"Most similar word and its similarity value: \", googlew2v.most_similar(positive=['dog', 'labrador'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2ce6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.77898705\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity: \", googlew2v.similarity('fantastic', 'amazing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63524839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity:  0.61599934\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity: \", googlew2v.similarity('king', 'prince'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d23681fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 similar words and their similarity values:  [('camry', 0.6310814023017883), ('chevy', 0.6251167058944702), ('camaro', 0.6154221892356873)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 3 similar words and their similarity values: \", googlew2v.most_similar(positive=['suv', 'honda'], negative=['cycle'], topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb6298b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5986675"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: Family − boy + girl = mother\n",
    "# calculating cosine similarity and then using it to calculate similarity between 2 vectors\n",
    "subtraction = googlew2v['family'] - googlew2v['boy']\n",
    "addition = subtraction + googlew2v['girl']\n",
    "num = np.dot(addition, googlew2v['mother'] )\n",
    "denom = (np.linalg.norm(addition)* np.linalg.norm(googlew2v['mother']))\n",
    "dist_google = num/denom\n",
    "dist_google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586586b5",
   "metadata": {},
   "source": [
    "(b) Train a Word2Vec model using your own dataset. You will use these extracted features in the subsequent questions of this assignment. Set the embedding size to be 300 and the window size to be 11. You can also consider\n",
    "a minimum word count of 10. Check the semantic similarities for the same\n",
    "two examples in part (a). What do you conclude from comparing vectors\n",
    "generated by yourself and the pretrained model? Which of the Word2Vec\n",
    "models seems to encode semantic similarities between words better?\n",
    "For the rest of this assignment, use the pretrained “word2vec-googlenews-300” Word2Ve features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84f9e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from reference given in hw2 pdf\n",
    "from gensim import utils\n",
    "from gensim.test.utils import datapath\n",
    "combined['review_body']= [str(i) for i in combined['review_body']]            \n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in combined['review_body']:\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "309fb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training own model\n",
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec(sentences=MyCorpus(), vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bba64fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word and its similarity value (own Word2Vec model):  [('woman', 0.7054744362831116)]\n"
     ]
    }
   ],
   "source": [
    "#checking similarity between the 2 words using built in function\n",
    "print(\"Most similar word and its similarity value (own Word2Vec model): \", w2v.wv.most_similar(positive=['women', 'man'], negative=['animal'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4e51a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word and its similarity value (Google news):  [('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Most similar word and its similarity value (Google news): \", googlew2v.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0b671cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity value (own Word2Vec model):  0.49703205\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity value (own Word2Vec model): \", w2v.wv.similarity('sun', 'moon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af892824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity value (Google news):  0.42628342\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity value (Google news): \", googlew2v.similarity('sun', 'moon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73906a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756631"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example: Family − boy + girl = mother\n",
    "# calculating cosine similarity and then using it to calculate similarity between 2 vectors\n",
    "subtraction = w2v.wv['family'] - w2v.wv['boy']\n",
    "addition = subtraction + w2v.wv['girl']\n",
    "num2 = np.dot(addition, w2v.wv['mother'] )\n",
    "denom2 = (np.linalg.norm(addition)* np.linalg.norm(w2v.wv['mother']))\n",
    "dist_own = num2/denom2\n",
    "dist_own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a193c",
   "metadata": {},
   "source": [
    "The similarity values for my own model are sometimes less or more than the pretrained model depending on the example but there are a lot of vocabulary missing from my own model. Moreover, the vectors of words in my own model sometimes makes no sense. The pretrained model gives better \"most similar\" words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55918082",
   "metadata": {},
   "source": [
    "# 3. Simple models\n",
    "Using the Google pre-trained Word2Vec features, train a perceptron and an SVM model for the five class classification problem. For this purpose, use the average Word2Vec vectors for each review as the input feature (x =1 NPN i=1 Wifor are view with N words). Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values  Word2Vec and TF-IDF features.What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617160e",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0645a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted all ratings to int\n",
    "combined['star_rating']= [int(i) for i in combined['star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0003500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TFIDF vectorization for feature extraction\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "fitt = vectorizer.fit_transform(combined['review_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6b76548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data as 80% training data and 20% testing data\n",
    "xtrain_tfidf, xtest_tfidf, ytrain_tfidf, ytest_tfidf = train_test_split(fitt,combined['star_rating'],random_state=1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "57189632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pretrained TFIDF vectors through Perceptron model\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "tfidf_perc_mod = Perceptron(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3442874",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_perc_mod.fit(xtrain_tfidf, ytrain_tfidf)\n",
    "pred_perc_tfidf = tfidf_perc_mod.predict(xtest_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2252b3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using the TFIDF(Perceptron): 0.569\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy using the TFIDF(Perceptron):', accuracy_score(ytest_tfidf, pred_perc_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c93eea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pretrained TFIDF vectors through LinearSVC model\n",
    "from sklearn.svm import LinearSVC\n",
    "tfidf_svm_mod = LinearSVC(max_iter=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b968b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_svm_mod.fit(xtrain_tfidf, ytrain_tfidf)\n",
    "pred_svm_tfidf = tfidf_svm_mod.predict(xtest_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d389339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using the TFIDF(SVM): 0.65085\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy using the TFIDF(SVM):', accuracy_score(ytest_tfidf, pred_svm_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2eba40",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77f2e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to average Word2Vec vectors for each review and tackling NaN results\n",
    "def average(x, w2v):\n",
    "    count, summ, ty = 0, np.zeros(shape=(300,)), type(x)\n",
    "    if ty == str:\n",
    "        lst = x.split(' ')\n",
    "    elif ty == list:\n",
    "        lst = x\n",
    "    for w in lst:\n",
    "        if w in w2v:\n",
    "            word = w2v[w]\n",
    "            count, summ = count + 1, summ + word\n",
    "    if count == 0:\n",
    "        return summ\n",
    "    else:\n",
    "        return (summ / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fdccd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing Word2Vec vector for splitting\n",
    "w2vxdata = combined['review_body'].apply(lambda x: average(x, googlew2v))\n",
    "w2vxdata = np.array(w2vxdata.values.tolist())\n",
    "w2vydata = combined['star_rating']\n",
    "w2vydata = np.array(w2vydata.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80969bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data as 80% training data and 20% testing data\n",
    "xtrain_google, xtest_google, ytrain_google, ytest_google = train_test_split(w2vxdata,w2vydata,random_state=1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11173101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training pretrained Word2Vec embedding through Perceptron model\n",
    "google_perc_mod = Perceptron(random_state=0)\n",
    "google_perc_mod.fit(xtrain_google, ytrain_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b9d7d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_perc_google = google_perc_mod.predict(xtest_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7835ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using the pretrained Word2Vec model(Perceptron): 0.4835\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy using the pretrained Word2Vec model(Perceptron):', accuracy_score(ytest_google, pred_perc_google ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59cf24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pretrained Word2Vec embedding through Perceptron model\n",
    "google_svm_mod = LinearSVC(max_iter=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30981b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_svm_mod.fit(xtrain_google, ytrain_google)\n",
    "pred_svm_google = google_svm_mod.predict(xtest_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65574dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using the pretrained Word2Vec model(SVM): 0.5945\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy using the pretrained Word2Vec model(SVM):', accuracy_score(ytest_google, pred_svm_google))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445a68e",
   "metadata": {},
   "source": [
    "Accuracy for the TFIDF vectorization is more than Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db4525c",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks\n",
    "Using the Word2Vec features, train a feedforward multilayer perceptron network for classification. Consider a network with two hidden layers, each with\n",
    "50 and 10 nodes, respectively. You can use cross entropy loss and your own\n",
    "choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc.\n",
    "Part of getting good results is to select good values for these hyperparamters.\n",
    "You can also refer to the following tutorial to familiarize yourself:\n",
    "https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist\n",
    "Although the above tutorial is for image data but the concept of training\n",
    "an MLP is very similar to what we want to do.\n",
    "\n",
    "(a) To generate the input features, use the average Word2Vec vectors similar to\n",
    "the “Simple models” section and train the neural network. Report accuracy\n",
    "values on the testing split for your MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea07b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2575baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "# we have to overwrite len() and getitem() functions\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.data = xtrain\n",
    "        self.labels = ytrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]   \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b695098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtest, ytest):\n",
    "        self.data = xtest\n",
    "        self.labels = ytest\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0186c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrainDataset(xtrain_google, ytrain_google-1)\n",
    "test_data = TestDataset(xtest_google, ytest_google-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32c160be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising batch_size, valid_size\n",
    "num_workers, batch_size, valid_size = 0, 32, 0.2\n",
    "# converting data to torch.FloatTensor\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# defining samplers for obtaining training and validation batches\n",
    "train_sampler, valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe516373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom FNN \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer 1\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer 2\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer 3\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 300)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a7d6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer used\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1eb41112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.283530 \tValidation Loss: 0.317319\n",
      "Validation loss decreased (inf --> 0.317319).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.207887 \tValidation Loss: 0.283512\n",
      "Validation loss decreased (0.317319 --> 0.283512).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.098549 \tValidation Loss: 0.259687\n",
      "Validation loss decreased (0.283512 --> 0.259687).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.031042 \tValidation Loss: 0.248246\n",
      "Validation loss decreased (0.259687 --> 0.248246).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.000458 \tValidation Loss: 0.242145\n",
      "Validation loss decreased (0.248246 --> 0.242145).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.972072 \tValidation Loss: 0.231757\n",
      "Validation loss decreased (0.242145 --> 0.231757).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.932853 \tValidation Loss: 0.222425\n",
      "Validation loss decreased (0.231757 --> 0.222425).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.908597 \tValidation Loss: 0.217585\n",
      "Validation loss decreased (0.222425 --> 0.217585).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.891317 \tValidation Loss: 0.213544\n",
      "Validation loss decreased (0.217585 --> 0.213544).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.877228 \tValidation Loss: 0.209856\n",
      "Validation loss decreased (0.213544 --> 0.209856).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.862411 \tValidation Loss: 0.206636\n",
      "Validation loss decreased (0.209856 --> 0.206636).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.852515 \tValidation Loss: 0.203664\n",
      "Validation loss decreased (0.206636 --> 0.203664).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.842260 \tValidation Loss: 0.201181\n",
      "Validation loss decreased (0.203664 --> 0.201181).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.830667 \tValidation Loss: 0.198034\n",
      "Validation loss decreased (0.201181 --> 0.198034).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.821941 \tValidation Loss: 0.195727\n",
      "Validation loss decreased (0.198034 --> 0.195727).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.811856 \tValidation Loss: 0.193285\n",
      "Validation loss decreased (0.195727 --> 0.193285).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.804343 \tValidation Loss: 0.191529\n",
      "Validation loss decreased (0.193285 --> 0.191529).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.798134 \tValidation Loss: 0.189489\n",
      "Validation loss decreased (0.191529 --> 0.189489).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.789906 \tValidation Loss: 0.187667\n",
      "Validation loss decreased (0.189489 --> 0.187667).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.786103 \tValidation Loss: 0.186551\n",
      "Validation loss decreased (0.187667 --> 0.186551).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.778144 \tValidation Loss: 0.185078\n",
      "Validation loss decreased (0.186551 --> 0.185078).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.774564 \tValidation Loss: 0.183956\n",
      "Validation loss decreased (0.185078 --> 0.183956).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.768200 \tValidation Loss: 0.182965\n",
      "Validation loss decreased (0.183956 --> 0.182965).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.764111 \tValidation Loss: 0.182040\n",
      "Validation loss decreased (0.182965 --> 0.182040).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.761840 \tValidation Loss: 0.181428\n",
      "Validation loss decreased (0.182040 --> 0.181428).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.757878 \tValidation Loss: 0.180491\n",
      "Validation loss decreased (0.181428 --> 0.180491).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.753142 \tValidation Loss: 0.179846\n",
      "Validation loss decreased (0.180491 --> 0.179846).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.749955 \tValidation Loss: 0.178730\n",
      "Validation loss decreased (0.179846 --> 0.178730).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.747823 \tValidation Loss: 0.178299\n",
      "Validation loss decreased (0.178730 --> 0.178299).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.744259 \tValidation Loss: 0.177401\n",
      "Validation loss decreased (0.178299 --> 0.177401).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.741140 \tValidation Loss: 0.176680\n",
      "Validation loss decreased (0.177401 --> 0.176680).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.739769 \tValidation Loss: 0.176523\n",
      "Validation loss decreased (0.176680 --> 0.176523).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.736456 \tValidation Loss: 0.175965\n",
      "Validation loss decreased (0.176523 --> 0.175965).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.735313 \tValidation Loss: 0.175633\n",
      "Validation loss decreased (0.175965 --> 0.175633).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.732908 \tValidation Loss: 0.174756\n",
      "Validation loss decreased (0.175633 --> 0.174756).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.728682 \tValidation Loss: 0.174300\n",
      "Validation loss decreased (0.174756 --> 0.174300).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.727095 \tValidation Loss: 0.174149\n",
      "Validation loss decreased (0.174300 --> 0.174149).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.725503 \tValidation Loss: 0.173683\n",
      "Validation loss decreased (0.174149 --> 0.173683).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.724420 \tValidation Loss: 0.173942\n",
      "Epoch: 40 \tTraining Loss: 0.721939 \tValidation Loss: 0.173747\n",
      "Epoch: 41 \tTraining Loss: 0.720678 \tValidation Loss: 0.172963\n",
      "Validation loss decreased (0.173683 --> 0.172963).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.719394 \tValidation Loss: 0.172775\n",
      "Validation loss decreased (0.172963 --> 0.172775).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.717943 \tValidation Loss: 0.173257\n",
      "Epoch: 44 \tTraining Loss: 0.716050 \tValidation Loss: 0.172351\n",
      "Validation loss decreased (0.172775 --> 0.172351).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.714429 \tValidation Loss: 0.172020\n",
      "Validation loss decreased (0.172351 --> 0.172020).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.712585 \tValidation Loss: 0.172018\n",
      "Validation loss decreased (0.172020 --> 0.172018).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.710972 \tValidation Loss: 0.171357\n",
      "Validation loss decreased (0.172018 --> 0.171357).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.709417 \tValidation Loss: 0.171162\n",
      "Validation loss decreased (0.171357 --> 0.171162).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.708836 \tValidation Loss: 0.171316\n",
      "Epoch: 50 \tTraining Loss: 0.706041 \tValidation Loss: 0.171605\n",
      "Epoch: 51 \tTraining Loss: 0.704654 \tValidation Loss: 0.170363\n",
      "Validation loss decreased (0.171162 --> 0.170363).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.704432 \tValidation Loss: 0.170718\n",
      "Epoch: 53 \tTraining Loss: 0.704036 \tValidation Loss: 0.170554\n",
      "Epoch: 54 \tTraining Loss: 0.700363 \tValidation Loss: 0.170224\n",
      "Validation loss decreased (0.170363 --> 0.170224).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.700776 \tValidation Loss: 0.170178\n",
      "Validation loss decreased (0.170224 --> 0.170178).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.699606 \tValidation Loss: 0.170063\n",
      "Validation loss decreased (0.170178 --> 0.170063).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.699103 \tValidation Loss: 0.169592\n",
      "Validation loss decreased (0.170063 --> 0.169592).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.698719 \tValidation Loss: 0.170323\n",
      "Epoch: 59 \tTraining Loss: 0.697625 \tValidation Loss: 0.169342\n",
      "Validation loss decreased (0.169592 --> 0.169342).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.695581 \tValidation Loss: 0.169525\n",
      "Epoch: 61 \tTraining Loss: 0.692989 \tValidation Loss: 0.168990\n",
      "Validation loss decreased (0.169342 --> 0.168990).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.694320 \tValidation Loss: 0.168724\n",
      "Validation loss decreased (0.168990 --> 0.168724).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.692491 \tValidation Loss: 0.168858\n",
      "Epoch: 64 \tTraining Loss: 0.692057 \tValidation Loss: 0.168538\n",
      "Validation loss decreased (0.168724 --> 0.168538).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.691134 \tValidation Loss: 0.168617\n",
      "Epoch: 66 \tTraining Loss: 0.689283 \tValidation Loss: 0.168328\n",
      "Validation loss decreased (0.168538 --> 0.168328).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.686578 \tValidation Loss: 0.169276\n",
      "Epoch: 68 \tTraining Loss: 0.689837 \tValidation Loss: 0.167913\n",
      "Validation loss decreased (0.168328 --> 0.167913).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.687044 \tValidation Loss: 0.168103\n",
      "Epoch: 70 \tTraining Loss: 0.686801 \tValidation Loss: 0.168339\n",
      "Epoch: 71 \tTraining Loss: 0.686725 \tValidation Loss: 0.168357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 \tTraining Loss: 0.684611 \tValidation Loss: 0.168353\n",
      "Epoch: 73 \tTraining Loss: 0.684309 \tValidation Loss: 0.168199\n",
      "Epoch: 74 \tTraining Loss: 0.683142 \tValidation Loss: 0.167983\n",
      "Epoch: 75 \tTraining Loss: 0.680762 \tValidation Loss: 0.168337\n",
      "Epoch: 76 \tTraining Loss: 0.680892 \tValidation Loss: 0.167249\n",
      "Validation loss decreased (0.167913 --> 0.167249).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.680362 \tValidation Loss: 0.167816\n",
      "Epoch: 78 \tTraining Loss: 0.680028 \tValidation Loss: 0.167316\n",
      "Epoch: 79 \tTraining Loss: 0.679358 \tValidation Loss: 0.167248\n",
      "Validation loss decreased (0.167249 --> 0.167248).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.678520 \tValidation Loss: 0.167224\n",
      "Validation loss decreased (0.167248 --> 0.167224).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.677902 \tValidation Loss: 0.167936\n",
      "Epoch: 82 \tTraining Loss: 0.677249 \tValidation Loss: 0.167007\n",
      "Validation loss decreased (0.167224 --> 0.167007).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.675521 \tValidation Loss: 0.168096\n",
      "Epoch: 84 \tTraining Loss: 0.675564 \tValidation Loss: 0.167307\n",
      "Epoch: 85 \tTraining Loss: 0.674490 \tValidation Loss: 0.166934\n",
      "Validation loss decreased (0.167007 --> 0.166934).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.672709 \tValidation Loss: 0.167865\n",
      "Epoch: 87 \tTraining Loss: 0.673517 \tValidation Loss: 0.166742\n",
      "Validation loss decreased (0.166934 --> 0.166742).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.674649 \tValidation Loss: 0.167214\n",
      "Epoch: 89 \tTraining Loss: 0.673019 \tValidation Loss: 0.168255\n",
      "Epoch: 90 \tTraining Loss: 0.671031 \tValidation Loss: 0.166525\n",
      "Validation loss decreased (0.166742 --> 0.166525).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.670820 \tValidation Loss: 0.167938\n",
      "Epoch: 92 \tTraining Loss: 0.670605 \tValidation Loss: 0.166804\n",
      "Epoch: 93 \tTraining Loss: 0.668151 \tValidation Loss: 0.166818\n",
      "Epoch: 94 \tTraining Loss: 0.668928 \tValidation Loss: 0.167139\n",
      "Epoch: 95 \tTraining Loss: 0.668372 \tValidation Loss: 0.168603\n",
      "Epoch: 96 \tTraining Loss: 0.666899 \tValidation Loss: 0.167352\n",
      "Epoch: 97 \tTraining Loss: 0.666013 \tValidation Loss: 0.166048\n",
      "Validation loss decreased (0.166525 --> 0.166048).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.665806 \tValidation Loss: 0.169283\n",
      "Epoch: 99 \tTraining Loss: 0.666535 \tValidation Loss: 0.166890\n",
      "Epoch: 100 \tTraining Loss: 0.664908 \tValidation Loss: 0.166193\n"
     ]
    }
   ],
   "source": [
    "n_epochs, valid_loss_min = 100, np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    \n",
    "    # training model\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss = train_loss + loss.item()*data.size(0)\n",
    "        \n",
    "    # validating model \n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss = valid_loss + loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss, valid_loss  = train_loss/len(train_loader.dataset), valid_loss/len(valid_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e4e7b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model with lowest validation loss\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4bd04783",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model.to(\"cpu\")\n",
    "        outputs = model(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        correct = correct + (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da5f40ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Average Word2Vec vectors FNN model:  66.3\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for Average Word2Vec vectors FNN model: ', (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864abdd",
   "metadata": {},
   "source": [
    "(b)To generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "for each review as the input feature (x = [WT 1, ..., WT 10]) and train the neural\n",
    "network. Report the accuracy value on the testing split for your MLP model.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained in the “’Simple Models” section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6b1b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the input features, concatenate the first 10 Word2Vec vectors\n",
    "def concatenation(s, w2v):\n",
    "    a,b = 0,0 \n",
    "    if type(s) == list:\n",
    "        lst = s\n",
    "    if type(s) == str:\n",
    "        lst = s.split(' ')\n",
    "    leng = len(lst)\n",
    "    while (a < leng) & (b < 10):\n",
    "        if lst[a] in w2v:\n",
    "            wv = w2v[lst[a]]\n",
    "            if b != 0:\n",
    "                res = np.concatenate((res, wv))\n",
    "            else:\n",
    "                res = wv\n",
    "            a, b = a + 1, b + 1\n",
    "            #print(\"a: \",a)\n",
    "            #print(\"b: \",b)\n",
    "        else:\n",
    "            a = a + 1\n",
    "    #print(\"b final:\", b)\n",
    "    if b < 10:\n",
    "        if (10 - b) != 10:\n",
    "            res = np.concatenate((res, np.zeros(shape=(300*(10 - b), )))) \n",
    "        else:\n",
    "            res = np.zeros(shape=(300*(10 - b), ))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa261aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the Word2Vec vectors for splitting\n",
    "w2vx10data = combined['review_body'].apply(lambda x: concatenation(x, googlew2v))\n",
    "w2vx10data = np.array(w2vx10data.values.tolist())\n",
    "w2vy10data = combined['star_rating']\n",
    "w2vy10data = np.array(w2vy10data.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f71b5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data as 80% training data and 20% testing data\n",
    "xtrain10_google, xtest10_google, ytrain10_google, ytest10_google = train_test_split(w2vx10data,w2vy10data,random_state=1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18cbc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.data = xtrain\n",
    "        self.labels = ytrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7776fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtest, ytest):\n",
    "        self.data = xtest\n",
    "        self.labels = ytest\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3e5d02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train10_data = TrainDataset(xtrain10_google, ytrain10_google-1)\n",
    "test10_data = TestDataset(xtest10_google, ytest10_google-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88d11c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers, batch_size, valid_size  = 0, 32, 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train10_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler, valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train10_data, batch_size=batch_size,sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader = torch.utils.data.DataLoader(train10_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test10_data, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9f2ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom FNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the FNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer 1\n",
    "        self.fc1 = nn.Linear(3000, hidden_1)\n",
    "        # linear layer 2\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer 3\n",
    "        self.fc3 = nn.Linear(hidden_2, 5)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 3000)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model10 = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb1e6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer used\n",
    "optimizer = torch.optim.SGD(model10.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "215ef04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.260506 \tValidation Loss: 0.296781\n",
      "Validation loss decreased (inf --> 0.296781).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.044644 \tValidation Loss: 0.220601\n",
      "Validation loss decreased (0.296781 --> 0.220601).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.896868 \tValidation Loss: 0.200710\n",
      "Validation loss decreased (0.220601 --> 0.200710).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.845342 \tValidation Loss: 0.192679\n",
      "Validation loss decreased (0.200710 --> 0.192679).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.813451 \tValidation Loss: 0.186653\n",
      "Validation loss decreased (0.192679 --> 0.186653).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.790590 \tValidation Loss: 0.182170\n",
      "Validation loss decreased (0.186653 --> 0.182170).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.775066 \tValidation Loss: 0.179425\n",
      "Validation loss decreased (0.182170 --> 0.179425).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.761997 \tValidation Loss: 0.177023\n",
      "Validation loss decreased (0.179425 --> 0.177023).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.750059 \tValidation Loss: 0.175439\n",
      "Validation loss decreased (0.177023 --> 0.175439).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.739694 \tValidation Loss: 0.174131\n",
      "Validation loss decreased (0.175439 --> 0.174131).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.731407 \tValidation Loss: 0.173073\n",
      "Validation loss decreased (0.174131 --> 0.173073).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.722393 \tValidation Loss: 0.171998\n",
      "Validation loss decreased (0.173073 --> 0.171998).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.716686 \tValidation Loss: 0.170993\n",
      "Validation loss decreased (0.171998 --> 0.170993).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.708511 \tValidation Loss: 0.170373\n",
      "Validation loss decreased (0.170993 --> 0.170373).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.701397 \tValidation Loss: 0.169133\n",
      "Validation loss decreased (0.170373 --> 0.169133).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.696935 \tValidation Loss: 0.168920\n",
      "Validation loss decreased (0.169133 --> 0.168920).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.691269 \tValidation Loss: 0.168499\n",
      "Validation loss decreased (0.168920 --> 0.168499).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.684719 \tValidation Loss: 0.167780\n",
      "Validation loss decreased (0.168499 --> 0.167780).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.679013 \tValidation Loss: 0.168175\n",
      "Epoch: 20 \tTraining Loss: 0.673193 \tValidation Loss: 0.167153\n",
      "Validation loss decreased (0.167780 --> 0.167153).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.665874 \tValidation Loss: 0.166948\n",
      "Validation loss decreased (0.167153 --> 0.166948).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.662770 \tValidation Loss: 0.166680\n",
      "Validation loss decreased (0.166948 --> 0.166680).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.658224 \tValidation Loss: 0.166985\n",
      "Epoch: 24 \tTraining Loss: 0.653228 \tValidation Loss: 0.166784\n",
      "Epoch: 25 \tTraining Loss: 0.649802 \tValidation Loss: 0.166879\n",
      "Epoch: 26 \tTraining Loss: 0.646602 \tValidation Loss: 0.166523\n",
      "Validation loss decreased (0.166680 --> 0.166523).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.640350 \tValidation Loss: 0.166935\n",
      "Epoch: 28 \tTraining Loss: 0.635417 \tValidation Loss: 0.166949\n",
      "Epoch: 29 \tTraining Loss: 0.631338 \tValidation Loss: 0.166895\n",
      "Epoch: 30 \tTraining Loss: 0.629316 \tValidation Loss: 0.166741\n"
     ]
    }
   ],
   "source": [
    "n_epochs, valid_loss_min = 30, np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    # training model\n",
    "    model10.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model10(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss = train_loss + loss.item()*data.size(0)\n",
    "        \n",
    "    # validating model\n",
    "    model10.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model10(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss = valid_loss + loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss, valid_loss = train_loss/len(train_loader.dataset), valid_loss/len(valid_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss,valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        torch.save(model10.state_dict(), 'model10.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "68a7f57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model with lowest validation loss\n",
    "model10.load_state_dict(torch.load('model10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "790588f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model10.to(\"cpu\")\n",
    "        outputs = model10(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        correct = correct + (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de34cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 10 Word2Vec vectors FNN model:  67.015\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for 10 Word2Vec vectors FNN model: ', (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308ee95",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks\n",
    "\n",
    "Using the Word2Vec features, train a recurrent neural network (RNN) for\n",
    "classification. You can refer to the following tutorial to familiarize yourself:\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_\n",
    "tutorial.html\n",
    "(a) Train a simple RNN for sentiment analysis. You can consider an RNN cell\n",
    "with the hidden state size of 20. To feed your data into our RNN, limit\n",
    "the maximum review length to 20 by truncating longer reviews and padding\n",
    "shorter reviews with a null value (0). Report accuracy values on the testing\n",
    "split for your RNN model.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained with feedforward neural network models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "89852e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to feed your data into our RNN, limit the maximum review length to 20 \n",
    "#by truncating longer reviews and padding shorter reviews with a null value (0).\n",
    "def review20(s, rnnw2v):\n",
    "    a, b = 0, 0\n",
    "    if type(s) == str:\n",
    "        lst = s.split(' ')\n",
    "    elif type(s) == list:\n",
    "        lst = s\n",
    "    leng = len(lst)\n",
    "    while (b < 20) & (a < leng):\n",
    "        if lst[a] in rnnw2v:\n",
    "            if b != 0:\n",
    "                w2v = rnnw2v[lst[a]] \n",
    "                w2vm = np.vstack((w2vm, w2v))\n",
    "            else:\n",
    "                w2vm = rnnw2v[lst[a]]\n",
    "            a, b = a + 1, b + 1\n",
    "        else:\n",
    "            a = a + 1\n",
    "    if b != 0:\n",
    "        zeros = np.zeros(shape=(20-b, 300))\n",
    "        w2vm = np.vstack((w2vm, zeros))\n",
    "    if b < 20:\n",
    "        w2vm = np.zeros(shape=(20, 300))\n",
    "    return w2vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "143921c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vxdatarnn = combined2['review_body'].apply(lambda x: review20(x, googlew2v))\n",
    "w2vydatarnn = combined2['star_rating']\n",
    "w2vxdatarnn = np.array(w2vxdatarnn.values.tolist())\n",
    "w2vydatarnn = np.array(w2vydatarnn.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "946cf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into 80% training and 20% testing\n",
    "xtrainrnn_google, xtestrnn_google, ytrainrnn_google, ytestrnn_google = train_test_split(w2vxdatarnn,w2vydatarnn,random_state=1, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "613cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class TrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.data = xtrain\n",
    "        self.labels = ytrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "            \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a5e7613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, xtest, ytest):\n",
    "        self.data = xtest\n",
    "        self.labels = ytest\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7c520838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datarnn = TrainDataset(xtrainrnn_google, ytrainrnn_google-1)\n",
    "test_datarnn = TestDataset(xtestrnn_google, ytestrnn_google-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c14ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers, batch_size, valid_size = 0, 32, 0.2\n",
    "# convert data to torch.FloatTensor\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_datarnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler, valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_datarnn, batch_size=batch_size,sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_rnn = torch.utils.data.DataLoader(train_datarnn, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_datarnn, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "61c07f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custon RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# initialize RNN\n",
    "model_rnn = RNNModel(300, 20, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "870a7453",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer_rnn = torch.optim.Adam(model_rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a8b6805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.283001 \tValidation Loss: 0.317449\n",
      "Validation loss decreased (inf --> 0.317449).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.252116 \tValidation Loss: 0.307635\n",
      "Validation loss decreased (0.317449 --> 0.307635).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.217740 \tValidation Loss: 0.306188\n",
      "Validation loss decreased (0.307635 --> 0.306188).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.199817 \tValidation Loss: 0.300398\n",
      "Validation loss decreased (0.306188 --> 0.300398).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.198098 \tValidation Loss: 0.297399\n",
      "Validation loss decreased (0.300398 --> 0.297399).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.189712 \tValidation Loss: 0.299164\n",
      "Epoch: 7 \tTraining Loss: 1.192799 \tValidation Loss: 0.297447\n",
      "Epoch: 8 \tTraining Loss: 1.189216 \tValidation Loss: 0.296797\n",
      "Validation loss decreased (0.297399 --> 0.296797).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.184553 \tValidation Loss: 0.294477\n",
      "Validation loss decreased (0.296797 --> 0.294477).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.194137 \tValidation Loss: 0.295632\n",
      "Epoch: 11 \tTraining Loss: 1.184758 \tValidation Loss: 0.298907\n",
      "Epoch: 12 \tTraining Loss: 1.180209 \tValidation Loss: 0.297092\n",
      "Epoch: 13 \tTraining Loss: 1.180336 \tValidation Loss: 0.295236\n",
      "Epoch: 14 \tTraining Loss: 1.177964 \tValidation Loss: 0.303544\n",
      "Epoch: 15 \tTraining Loss: 1.176563 \tValidation Loss: 0.296384\n",
      "Epoch: 16 \tTraining Loss: 1.177640 \tValidation Loss: 0.300024\n",
      "Epoch: 17 \tTraining Loss: 1.175896 \tValidation Loss: 0.299347\n",
      "Epoch: 18 \tTraining Loss: 1.177329 \tValidation Loss: 0.310685\n",
      "Epoch: 19 \tTraining Loss: 1.172469 \tValidation Loss: 0.295979\n",
      "Epoch: 20 \tTraining Loss: 1.176689 \tValidation Loss: 0.294404\n",
      "Validation loss decreased (0.294477 --> 0.294404).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.175606 \tValidation Loss: 0.301625\n",
      "Epoch: 22 \tTraining Loss: 1.183474 \tValidation Loss: 0.314457\n",
      "Epoch: 23 \tTraining Loss: 1.197302 \tValidation Loss: 0.304831\n",
      "Epoch: 24 \tTraining Loss: 1.175817 \tValidation Loss: 0.299629\n",
      "Epoch: 25 \tTraining Loss: 1.172117 \tValidation Loss: 0.294882\n",
      "Epoch: 26 \tTraining Loss: 1.184060 \tValidation Loss: 0.298823\n",
      "Epoch: 27 \tTraining Loss: 1.171910 \tValidation Loss: 0.294755\n",
      "Epoch: 28 \tTraining Loss: 1.172655 \tValidation Loss: 0.298792\n",
      "Epoch: 29 \tTraining Loss: 1.170841 \tValidation Loss: 0.296271\n",
      "Epoch: 30 \tTraining Loss: 1.172918 \tValidation Loss: 0.297978\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs, valid_loss_min = 30, np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "\n",
    "    # training the model\n",
    "    model_rnn.train() # prep model for training\n",
    "    for data, target in train_loader_rnn:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_rnn.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_rnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_rnn.step()\n",
    "        # update running training loss\n",
    "        train_loss = train_loss + loss.item()*data.size(0)\n",
    "\n",
    "    #validating model\n",
    "    model_rnn.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_rnn:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_rnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss\n",
    "        valid_loss = valid_loss + loss.item()*data.size(0)\n",
    "\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss, valid_loss = train_loss/len(train_loader_rnn.dataset), valid_loss/len(valid_loader_rnn.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        torch.save(model_rnn.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c1706c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model with lowest validation loss\n",
    "model_rnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "233edcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_rnn:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_rnn.to(\"cpu\")\n",
    "        outputs = model_rnn(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = total + labels.size(0)\n",
    "        correct = correct + (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6d37b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for RNN:  33.175\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for RNN: ', (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a9af2",
   "metadata": {},
   "source": [
    "(b) Repeat part (a) by considering a gated recurrent unit cell.\n",
    "What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained using simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cb33bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers, batch_size, valid_size = 0, 32, 0.2\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_datarnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler,valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx)\n",
    "# prepare data loaders\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_datarnn, batch_size=batch_size,sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_rnn = torch.utils.data.DataLoader(train_datarnn, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_datarnn, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb130c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom GatedRNN\n",
    "class GatedRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # GRU\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim)\n",
    "        # One time step\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "model_gatedrnn = GatedRNN(300, 20, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b7441ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function used\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer used for gradient descent\n",
    "optimizer_rnn = torch.optim.SGD(model_gatedrnn.parameters(), lr=0.0075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b9beff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.289548 \tValidation Loss: 0.321773\n",
      "Validation loss decreased (inf --> 0.321773).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.285296 \tValidation Loss: 0.321020\n",
      "Validation loss decreased (0.321773 --> 0.321020).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.281657 \tValidation Loss: 0.320157\n",
      "Validation loss decreased (0.321020 --> 0.320157).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.277183 \tValidation Loss: 0.319084\n",
      "Validation loss decreased (0.320157 --> 0.319084).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.270888 \tValidation Loss: 0.317087\n",
      "Validation loss decreased (0.319084 --> 0.317087).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.257829 \tValidation Loss: 0.311675\n",
      "Validation loss decreased (0.317087 --> 0.311675).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.232900 \tValidation Loss: 0.305766\n",
      "Validation loss decreased (0.311675 --> 0.305766).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.217541 \tValidation Loss: 0.303540\n",
      "Validation loss decreased (0.305766 --> 0.303540).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.207840 \tValidation Loss: 0.300587\n",
      "Validation loss decreased (0.303540 --> 0.300587).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.199622 \tValidation Loss: 0.298878\n",
      "Validation loss decreased (0.300587 --> 0.298878).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.192036 \tValidation Loss: 0.296773\n",
      "Validation loss decreased (0.298878 --> 0.296773).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.184700 \tValidation Loss: 0.296729\n",
      "Validation loss decreased (0.296773 --> 0.296729).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.176564 \tValidation Loss: 0.292547\n",
      "Validation loss decreased (0.296729 --> 0.292547).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.168211 \tValidation Loss: 0.291987\n",
      "Validation loss decreased (0.292547 --> 0.291987).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.161736 \tValidation Loss: 0.289605\n",
      "Validation loss decreased (0.291987 --> 0.289605).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.156584 \tValidation Loss: 0.292754\n",
      "Epoch: 17 \tTraining Loss: 1.152186 \tValidation Loss: 0.287800\n",
      "Validation loss decreased (0.289605 --> 0.287800).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.148037 \tValidation Loss: 0.289288\n",
      "Epoch: 19 \tTraining Loss: 1.145159 \tValidation Loss: 0.287960\n",
      "Epoch: 20 \tTraining Loss: 1.141931 \tValidation Loss: 0.285502\n",
      "Validation loss decreased (0.287800 --> 0.285502).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.138719 \tValidation Loss: 0.284989\n",
      "Validation loss decreased (0.285502 --> 0.284989).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.135665 \tValidation Loss: 0.289896\n",
      "Epoch: 23 \tTraining Loss: 1.133620 \tValidation Loss: 0.286784\n",
      "Epoch: 24 \tTraining Loss: 1.130854 \tValidation Loss: 0.283358\n",
      "Validation loss decreased (0.284989 --> 0.283358).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.128849 \tValidation Loss: 0.283733\n",
      "Epoch: 26 \tTraining Loss: 1.125378 \tValidation Loss: 0.282474\n",
      "Validation loss decreased (0.283358 --> 0.282474).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.123925 \tValidation Loss: 0.282077\n",
      "Validation loss decreased (0.282474 --> 0.282077).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.121677 \tValidation Loss: 0.281045\n",
      "Validation loss decreased (0.282077 --> 0.281045).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.119699 \tValidation Loss: 0.287675\n",
      "Epoch: 30 \tTraining Loss: 1.117810 \tValidation Loss: 0.281724\n",
      "Epoch: 31 \tTraining Loss: 1.115807 \tValidation Loss: 0.280709\n",
      "Validation loss decreased (0.281045 --> 0.280709).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.113873 \tValidation Loss: 0.279994\n",
      "Validation loss decreased (0.280709 --> 0.279994).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.112199 \tValidation Loss: 0.279635\n",
      "Validation loss decreased (0.279994 --> 0.279635).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.110301 \tValidation Loss: 0.282371\n",
      "Epoch: 35 \tTraining Loss: 1.108387 \tValidation Loss: 0.281459\n",
      "Epoch: 36 \tTraining Loss: 1.106926 \tValidation Loss: 0.278647\n",
      "Validation loss decreased (0.279635 --> 0.278647).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.105597 \tValidation Loss: 0.278015\n",
      "Validation loss decreased (0.278647 --> 0.278015).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.103633 \tValidation Loss: 0.280401\n",
      "Epoch: 39 \tTraining Loss: 1.102408 \tValidation Loss: 0.281914\n",
      "Epoch: 40 \tTraining Loss: 1.100871 \tValidation Loss: 0.278361\n",
      "Epoch: 41 \tTraining Loss: 1.099510 \tValidation Loss: 0.276755\n",
      "Validation loss decreased (0.278015 --> 0.276755).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.098190 \tValidation Loss: 0.277570\n",
      "Epoch: 43 \tTraining Loss: 1.096485 \tValidation Loss: 0.276254\n",
      "Validation loss decreased (0.276755 --> 0.276254).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.095302 \tValidation Loss: 0.276883\n",
      "Epoch: 45 \tTraining Loss: 1.093840 \tValidation Loss: 0.275849\n",
      "Validation loss decreased (0.276254 --> 0.275849).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.092995 \tValidation Loss: 0.275682\n",
      "Validation loss decreased (0.275849 --> 0.275682).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.091815 \tValidation Loss: 0.275710\n",
      "Epoch: 48 \tTraining Loss: 1.090409 \tValidation Loss: 0.275230\n",
      "Validation loss decreased (0.275682 --> 0.275230).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.089028 \tValidation Loss: 0.274874\n",
      "Validation loss decreased (0.275230 --> 0.274874).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.088055 \tValidation Loss: 0.276718\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs, valid_loss_min = 50, np.Inf\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "    train_loss, valid_loss = 0.0, 0.0\n",
    "    #training model\n",
    "    model_gatedrnn.train() # prep model for training\n",
    "    for data, target in train_loader_rnn:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_rnn.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gatedrnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_rnn.step()\n",
    "        # update running training loss\n",
    "        train_loss = train_loss + loss.item()*data.size(0)\n",
    "\n",
    "    # validating model\n",
    "    model_gatedrnn.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_rnn:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_gatedrnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss\n",
    "        valid_loss = valid_loss + loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss, valid_loss = train_loss/len(train_loader_rnn.dataset), valid_loss/len(valid_loader_rnn.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        torch.save(model_gatedrnn.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f2aab67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model with the lowest validation loss\n",
    "model_gatedrnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "73517516",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_rnn:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_gatedrnn.to(\"cpu\")\n",
    "        outputs = model_gatedrnn(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e81ab052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Gated RNN model:  39.705\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for Gated RNN model: ', (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb195f",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6719ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853233cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
